# Distributed System Platform

Платформа для экспериментов с распределёнными системами, позволяет запускать распределённые приложения, генерировать трафик и намеренно создавать сбои.

## Ключевые требования

- Запуск кода на разных языках. Код компонентов системы должен выполняться в окружении, близком к реальному
- Сетевое взаимодействие между узлами. Должна поддерживаться связь между процессами/узлами (например, по TCP/UDP) как в реальных распределённых системах.
- Моделирование отказов и сбоев. Необходимо уметь имитировать различные сбои: сетевые проблемы (задержки, потери пакетов, «разрывы» сети), отказы узлов (выключение/перезапуск), деградацию производительности (например, узел начинает отвечать медленно при высокой нагрузке)
- Сбор метрик и наблюдаемость. Платформа должна предоставлять способ наблюдать за системой: собирать метрики, логи, визуализировать их, чтобы пользователи могли выявлять проблемы и анализировать поведение системы
- Генерация трафика. Платформа должна поддерживать управляемую генерацию трафика для распределённых приложений по различным сценариям, характерным для реальных систем: постоянная нагрузка, плавное изменение нагрузки, всплески, пакетная и стохастическая нагрузка
- Максимальная приближенность к реальности. Инфраструктура и способы работы со сбоями должны быть такими, с которыми пользователи затем столкнутся на практике. Решения, применённые на платформе, должны переноситься на реальные кластеры и сервисы
- Простота развертывания. Платформа должна работать локально (на компьютере пользователя). Индивидуальный режим использования (каждый пользователь поднимает свой стенд) для упрощения экспериментов
- Удобство использования. Необходимо наличие простого Web API для управления отказами, чтобы можно было в реальном времени моделировать разные варианты отказов узлов и сети

## Сценарий использования

1. Пользователь запускает платформу
2. Пользователь запускает генерацию трафика в распределенную систему
3. Пользователь "включает" различные виды отказов на платформе (отказы узлов, обрыв сети, уменьшение пропускной способности и т.п.)
4. Пользователь анализирует поведение системы по метрикам и трейсам
5. Пользователь повторяет пункты 2-4 для моделирования различных ситуаций

## Реализация

Использовать Docker-контейнеры для каждого узла (клиента, сервера и пр.), а для управления сбоями - существующие утилиты.

### Организация узлов

Для каждого компонента системы запускается отдельный контейнер используя Dockerfile. Контейнеры объединяются в одну виртуальную сеть Docker, чтобы они могли общаться, строится общий Docker Compose.

### Управление отказами

Используется готовая утилита Pumba + Web API в отдельном контейнере для управления отказами на лету.

### Наблюдаемость

В Docker-окружении платформы запускаются контейнеры OpenTelemetry Collector, Prometheus, Grafana и Grafana Tempo. Каждый компонент распределённой системы использует OpenTelemetry SDK и отправляет телеметрию через OTLP.
Сбор метрик:
Сервисы → OTLP → OpenTelemetry Collector → Prometheus exporter endpoint → Prometheus (scrape)
Сбор трейсов:
Сервисы → OTLP → OpenTelemetry Collector → Grafana Tempo
Такой подход позволяет в реальном времени наблюдать метрики приложения (задержки, ошибки, runtime-метрики), а также распределённые трейсы запросов через несколько узлов системы.

### Генерация трафика

Генерация трафика реализуется через запуск k6 как job-контейнера.  
Управление осуществляется внешним Platform API, который запускает и останавливает контейнеры k6 с заданными сценариями и параметрами.  
Метрики нагрузки и результатов теста экспортируются в Prometheus и визуализируются в Grafana.

### Схема контейнеров платформы

```mermaid
flowchart TB
  User{{User}}
  Docker["Docker Engine"]

  subgraph DistributedSystem["Distributed System"]
    Node1["Node1"]
    Node2["Node2"]
    Node3["Node3"]

    Node1 <-->|api| Node2
    Node2 <-->|api| Node3
    Node1 <-->|api| Node3
  end

  subgraph Management["Management"]
    Api["Platform WebAPI + swagger"]
    K6["K6 (traffic generator)"]
    Pumba["Pumba (failure management)"]
    Prometheus[(Prometheus)]
    OtelCol["OpenTelemetry Collector"]
    Grafana["Grafana"]
    Tempo[("Tempo")]

    Api -->|"run job-container"| Pumba
    Api -->|"run job-container"| K6
    Grafana -->|"query (metrics)"| Prometheus
    Grafana -->|"query (traces)"| Tempo
    K6 -->|"Remote write"| Prometheus
    OtelCol -->|"Prometheus exporter endpoint"| Prometheus
    OtelCol -->|"OTLP (traces)"| Tempo
  end

  DistributedSystem -->|"OTLP (metrics + traces)"| OtelCol
  Pumba -->|"Control containers"| Docker
  Docker -->|"Control containers"| DistributedSystem
  K6 -->|Generate traffic| DistributedSystem

  User -->|"Control"| Api
  User -->|"View dashboards"| Grafana
  ```

### Описание контейнеров (Management)

#### Platform API (WebAPI + Swagger)

Центральная точка управления платформой и параметрами среды
- REST API с Swagger UI для интерактивного управления
- Имеет доступ к Docker Engine (через `/var/run/docker.sock`)
- Управляет запуском и остановкой k6 job-контейнеров
- Управляет включением и выключением отказов через Pumba

#### Pumba (Failure Management)

Моделирование отказов контейнеров и сети
- Запускается как job-контейнер
- Имеет доступ к Docker API
- Управляется Platform API
- Поддерживает:
  - сбои контейнеров (pause, stop, kill, restart)
  - сетевые сбои (latency, jitter, packet loss, duplication, corruption)
  - ограничение пропускной способности
- Использует лейблы и имена контейнеров для точечного применения отказов

#### K6 (Traffic Generator)

Генерация нагрузки на распределённую систему
- Запускается как job-контейнер
- Управляется Platform API
- Поддерживает различные сценарии нагрузки:
  - постоянная
  - плавное увеличение/уменьшение
  - всплески
  - стохастическая и пакетная нагрузка
- Генерирует трафик внутри Docker-сети
- Экспортирует метрики нагрузки и результатов тестов в Prometheus

#### OpenTelemetry Collector

Централизованный сбор и маршрутизация телеметрии
- Принимает метрики и трейсы по OTLP (gRPC/HTTP) от всех узлов
- Экспортирует метрики через Prometheus exporter endpoint
- Экспортирует трейсы в Grafana Tempo
- Обогащает телеметрию ресурсными атрибутами (`service.name`, `container`, `node`)

#### Prometheus

Хранение и запрос метрик платформы и приложений
- Принимает метрики сервисов от OpenTelemetry Collector
- Принимает метрики нагрузки от k6
- Хранит таймсерии локально (volume)
- Использует лейблы для разделения метрик приложений, нагрузки и инфраструктуры

#### Grafana

Визуализация метрик и распределённых трейсов
- Подключена к Prometheus (метрики) и Tempo (трейсы)
- Содержит преднастроенные дашборды:
  - нагрузка и производительность,
  - ошибки и задержки,
  - устойчивость к сбоям и деградации,
  - учебные метрики (потери, дубликаты, exactly-once)
- Поддерживает provisioning дашбордов и datasource
- Хранит состояние и конфигурацию в volume

#### Tempo

Хранение и анализа распределённых трейсов
- Принимает трейсы от OpenTelemetry Collector
- Хранит трейсы локально (volume)
- Интегрирован с Grafana для просмотра цепочек вызовов и поиска по trace/span

## Демонстрация

Для демонстрации работы платформы реализованы два сервиса

- ServiceA
  - Port: 10001
  - API
    - POST: /api/message-a
    - POST: /api/error
- ServiceB
  - Port: 10002
  - API
    - POST: /api/message-b

```mermaid
flowchart LR
  Client{{Client}}
  ServiceA["ServiceA"]
  ServiceB["ServiceB"]

  Client -->|/api/message-a| ServiceA
  ServiceA -->|/api/message-b| ServiceB
```

### Быстрый запуск

1. Запустить платформу `docker compose up -d`
2. Запустить генерацию траффика
```
POST: http://localhost:5050/api/traffic/start
{
  "targetUrl": "http://service-a/api/message-a",
  "rps": 10,
  "durationSeconds": 300
}
```
3. Наблюдать телеметрию сервисов в Grafana по адресу: `http://localhost:3000/d/dsp-service-detail/service-detail`